#!/usr/bin/env python3
"""
Final System Validation for PDE-Fluid-Œ¶

Comprehensive end-to-end validation of the complete system including:
- All implemented components and innovations
- Integration between subsystems  
- Production readiness verification
- Performance benchmarking
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime


class SystemValidator:
    """
    Comprehensive system validation for the complete PDE-Fluid-Œ¶ implementation.
    """
    
    def __init__(self, project_root: str = "/root/repo"):
        self.project_root = Path(project_root)
        self.logger = self._setup_logger()
        
        # Validation results
        self.validation_results = {}
        self.overall_score = 0.0
        
    def _setup_logger(self) -> logging.Logger:
        """Setup validation logging."""
        logger = logging.getLogger('system_validator')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - VALIDATION - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def run_comprehensive_validation(self) -> Dict[str, Any]:
        """Run comprehensive end-to-end system validation."""
        
        self.logger.info("üîç Starting Comprehensive System Validation")
        self.logger.info("=" * 60)
        
        start_time = time.time()
        
        validation_components = [
            ('project_structure', self._validate_project_structure),
            ('core_innovations', self._validate_core_innovations), 
            ('advanced_models', self._validate_advanced_models),
            ('optimization_systems', self._validate_optimization_systems),
            ('quality_assurance', self._validate_quality_assurance),
            ('deployment_readiness', self._validate_deployment_readiness),
            ('documentation_completeness', self._validate_documentation),
            ('system_integration', self._validate_system_integration)
        ]
        
        # Run all validation components
        for component_name, validator_func in validation_components:
            try:
                self.logger.info(f"üîß Validating: {component_name.replace('_', ' ').title()}")
                
                component_result = validator_func()
                self.validation_results[component_name] = component_result
                
                if component_result['passed']:
                    self.logger.info(f"   ‚úÖ {component_name} - PASSED (score: {component_result['score']:.3f})")
                else:
                    self.logger.warning(f"   ‚ùå {component_name} - FAILED (score: {component_result['score']:.3f})")
                    
            except Exception as e:
                self.logger.error(f"   üí• {component_name} - ERROR: {e}")
                self.validation_results[component_name] = {
                    'passed': False,
                    'score': 0.0,
                    'error': str(e)
                }
        
        total_time = time.time() - start_time
        
        # Compute overall score
        self._compute_overall_score()
        
        # Generate final report
        final_report = self._generate_final_report(total_time)
        
        # Save results
        self._save_validation_results(final_report)
        
        # Log summary
        self._log_validation_summary(final_report)
        
        return final_report
    
    def _validate_project_structure(self) -> Dict[str, Any]:
        """Validate overall project structure."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check critical directories
        critical_dirs = [
            'src/pde_fluid_phi',
            'deployment', 
            'examples',
            'tests',
            'quality_reports'
        ]
        
        present_dirs = []
        for dir_path in critical_dirs:
            full_path = self.project_root / dir_path
            if full_path.exists() and full_path.is_dir():
                present_dirs.append(dir_path)
        
        # Check critical files
        critical_files = [
            'README.md',
            'requirements.txt', 
            'pyproject.toml',
            'Dockerfile',
            'FINAL_IMPLEMENTATION_SUMMARY.md'
        ]
        
        present_files = []
        for file_path in critical_files:
            full_path = self.project_root / file_path
            if full_path.exists() and full_path.is_file():
                present_files.append(file_path)
        
        # Calculate structure score
        dir_score = len(present_dirs) / len(critical_dirs)
        file_score = len(present_files) / len(critical_files)
        structure_score = (dir_score + file_score) / 2
        
        result['score'] = structure_score
        result['passed'] = structure_score >= 0.8
        result['details'] = {
            'directories': {
                'present': present_dirs,
                'missing': [d for d in critical_dirs if d not in present_dirs]
            },
            'files': {
                'present': present_files,
                'missing': [f for f in critical_files if f not in present_files]
            }
        }
        
        return result
    
    def _validate_core_innovations(self) -> Dict[str, Any]:
        """Validate core innovations are implemented."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check for core innovation files
        core_innovations = {
            'rational_fourier_operators': 'src/pde_fluid_phi/operators/rational_fourier.py',
            'quantum_enhancements': 'src/pde_fluid_phi/operators/quantum_rational_fourier.py',
            'stability_mechanisms': 'src/pde_fluid_phi/operators/stability.py',
            'multiphysics_coupling': 'src/pde_fluid_phi/physics/multiphysics_coupling.py'
        }
        
        implemented_innovations = {}
        
        for innovation_name, file_path in core_innovations.items():
            full_path = self.project_root / file_path
            
            if full_path.exists():
                # Check file has substantial content (not just placeholder)
                try:
                    content = full_path.read_text()
                    has_substantial_content = len(content) > 1000  # At least 1KB
                    has_class_definitions = 'class ' in content
                    has_forward_method = 'def forward(' in content
                    
                    implemented_innovations[innovation_name] = {
                        'file_exists': True,
                        'substantial_content': has_substantial_content,
                        'has_classes': has_class_definitions,
                        'has_forward_method': has_forward_method,
                        'implemented': has_substantial_content and has_class_definitions
                    }
                except Exception as e:
                    implemented_innovations[innovation_name] = {
                        'file_exists': True,
                        'error': str(e),
                        'implemented': False
                    }
            else:
                implemented_innovations[innovation_name] = {
                    'file_exists': False,
                    'implemented': False
                }
        
        # Calculate innovation score
        implementation_scores = [
            details.get('implemented', False) for details in implemented_innovations.values()
        ]
        innovation_score = sum(implementation_scores) / len(implementation_scores)
        
        result['score'] = innovation_score
        result['passed'] = innovation_score >= 0.75
        result['details'] = implemented_innovations
        
        return result
    
    def _validate_advanced_models(self) -> Dict[str, Any]:
        """Validate advanced model implementations."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check for advanced model files
        advanced_models = {
            'rational_fno': 'src/pde_fluid_phi/models/rfno.py',
            'self_healing_fno': 'src/pde_fluid_phi/models/self_healing_rfno.py',
            'multiscale_fno': 'src/pde_fluid_phi/models/multiscale_fno.py',
            'base_fno': 'src/pde_fluid_phi/models/fno3d.py'
        }
        
        model_implementations = {}
        
        for model_name, file_path in advanced_models.items():
            full_path = self.project_root / file_path
            
            if full_path.exists():
                try:
                    content = full_path.read_text()
                    has_model_class = f'class {model_name.upper().replace("_", "")}' in content or 'class ' in content
                    has_forward_method = 'def forward(' in content
                    has_substantial_content = len(content) > 2000  # Advanced models should be substantial
                    
                    model_implementations[model_name] = {
                        'exists': True,
                        'has_class': has_model_class,
                        'has_forward': has_forward_method,
                        'substantial': has_substantial_content,
                        'implemented': has_model_class and has_forward_method and has_substantial_content
                    }
                except Exception as e:
                    model_implementations[model_name] = {
                        'exists': True,
                        'error': str(e),
                        'implemented': False
                    }
            else:
                model_implementations[model_name] = {
                    'exists': False,
                    'implemented': False
                }
        
        # Calculate model score
        model_scores = [details.get('implemented', False) for details in model_implementations.values()]
        model_score = sum(model_scores) / len(model_scores)
        
        result['score'] = model_score
        result['passed'] = model_score >= 0.75
        result['details'] = model_implementations
        
        return result
    
    def _validate_optimization_systems(self) -> Dict[str, Any]:
        """Validate optimization and scaling systems."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check for optimization system files
        optimization_systems = {
            'evolutionary_nas': 'src/pde_fluid_phi/optimization/evolutionary_nas.py',
            'extreme_scale_optimizer': 'src/pde_fluid_phi/optimization/extreme_scale_optimizer.py',
            'performance_optimization': 'src/pde_fluid_phi/optimization/performance_optimization.py',
            'distributed_training': 'src/pde_fluid_phi/optimization/distributed_training.py'
        }
        
        optimization_implementations = {}
        
        for system_name, file_path in optimization_systems.items():
            full_path = self.project_root / file_path
            
            if full_path.exists():
                try:
                    content = full_path.read_text()
                    has_optimizer_class = 'class ' in content and ('Optimizer' in content or 'NAS' in content)
                    has_optimization_methods = 'optimize' in content.lower() or 'evolve' in content.lower()
                    has_substantial_content = len(content) > 3000  # Optimization systems are complex
                    
                    optimization_implementations[system_name] = {
                        'exists': True,
                        'has_class': has_optimizer_class,
                        'has_methods': has_optimization_methods,
                        'substantial': has_substantial_content,
                        'implemented': has_optimizer_class and has_optimization_methods
                    }
                except Exception as e:
                    optimization_implementations[system_name] = {
                        'exists': True,
                        'error': str(e),
                        'implemented': False
                    }
            else:
                optimization_implementations[system_name] = {
                    'exists': False,
                    'implemented': False
                }
        
        # Calculate optimization score
        optimization_scores = [details.get('implemented', False) for details in optimization_implementations.values()]
        optimization_score = sum(optimization_scores) / len(optimization_scores)
        
        result['score'] = optimization_score
        result['passed'] = optimization_score >= 0.6  # Lower threshold as these are advanced features
        result['details'] = optimization_implementations
        
        return result
    
    def _validate_quality_assurance(self) -> Dict[str, Any]:
        """Validate quality assurance systems."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check for quality assurance files
        qa_files = [
            'simplified_quality_gates.py',
            'advanced_quality_gates.py',
            'quality_reports'
        ]
        
        qa_status = {}
        
        for qa_file in qa_files:
            full_path = self.project_root / qa_file
            
            if full_path.exists():
                if qa_file.endswith('.py'):
                    try:
                        content = full_path.read_text()
                        has_quality_logic = 'quality' in content.lower() and 'gate' in content.lower()
                        is_substantial = len(content) > 1000
                        
                        qa_status[qa_file] = {
                            'exists': True,
                            'functional': has_quality_logic and is_substantial
                        }
                    except Exception as e:
                        qa_status[qa_file] = {
                            'exists': True,
                            'error': str(e),
                            'functional': False
                        }
                else:  # Directory
                    qa_status[qa_file] = {
                        'exists': True,
                        'functional': len(list(full_path.glob('*'))) > 0 if full_path.is_dir() else False
                    }
            else:
                qa_status[qa_file] = {
                    'exists': False,
                    'functional': False
                }
        
        # Try to run quality gates if available
        quality_script = self.project_root / "simplified_quality_gates.py"
        if quality_script.exists():
            try:
                import subprocess
                result_proc = subprocess.run(
                    [sys.executable, str(quality_script)],
                    cwd=str(self.project_root),
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                qa_status['quality_gates_execution'] = {
                    'ran_successfully': result_proc.returncode == 0,
                    'output_length': len(result_proc.stdout),
                    'functional': result_proc.returncode == 0
                }
                
            except Exception as e:
                qa_status['quality_gates_execution'] = {
                    'ran_successfully': False,
                    'error': str(e),
                    'functional': False
                }
        
        # Calculate QA score
        functional_items = sum(1 for item in qa_status.values() if item.get('functional', False))
        qa_score = functional_items / len(qa_status)
        
        result['score'] = qa_score
        result['passed'] = qa_score >= 0.7
        result['details'] = qa_status
        
        return result
    
    def _validate_deployment_readiness(self) -> Dict[str, Any]:
        """Validate deployment readiness."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check deployment components
        deployment_components = {
            'dockerfile': 'Dockerfile',
            'docker_compose': 'docker-compose.yml',
            'deployment_scripts': 'deployment/',
            'production_orchestrator': 'deployment/production_orchestration.py',
            'deployment_config': 'deployment/production_config.yaml',
            'kubernetes_manifests': 'deployment/kubernetes/'
        }
        
        deployment_status = {}
        
        for component_name, component_path in deployment_components.items():
            full_path = self.project_root / component_path
            
            if full_path.exists():
                if full_path.is_file():
                    try:
                        content = full_path.read_text()
                        is_substantial = len(content) > 100
                        deployment_status[component_name] = {
                            'exists': True,
                            'substantial': is_substantial,
                            'ready': is_substantial
                        }
                    except Exception:
                        deployment_status[component_name] = {
                            'exists': True,
                            'substantial': False,
                            'ready': False
                        }
                else:  # Directory
                    has_files = len(list(full_path.glob('*'))) > 0
                    deployment_status[component_name] = {
                        'exists': True,
                        'has_files': has_files,
                        'ready': has_files
                    }
            else:
                deployment_status[component_name] = {
                    'exists': False,
                    'ready': False
                }
        
        # Test deployment script if available
        deploy_script = self.project_root / "deployment/deploy_production.py"
        if deploy_script.exists():
            try:
                import subprocess
                result_proc = subprocess.run(
                    [sys.executable, str(deploy_script), "--environment", "staging", "--dry-run"],
                    cwd=str(self.project_root),
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                deployment_status['deployment_script_test'] = {
                    'executable': result_proc.returncode == 0,
                    'ready': result_proc.returncode == 0
                }
                
            except Exception as e:
                deployment_status['deployment_script_test'] = {
                    'executable': False,
                    'error': str(e),
                    'ready': False
                }
        
        # Calculate deployment score
        ready_components = sum(1 for item in deployment_status.values() if item.get('ready', False))
        deployment_score = ready_components / len(deployment_status)
        
        result['score'] = deployment_score
        result['passed'] = deployment_score >= 0.7
        result['details'] = deployment_status
        
        return result
    
    def _validate_documentation(self) -> Dict[str, Any]:
        """Validate documentation completeness."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        # Check documentation files
        doc_files = {
            'main_readme': 'README.md',
            'implementation_summary': 'FINAL_IMPLEMENTATION_SUMMARY.md',
            'architecture_docs': 'ARCHITECTURE.md',
            'comprehensive_docs': 'COMPREHENSIVE_DOCUMENTATION.md',
            'changelog': 'CHANGELOG.md',
            'contributing': 'CONTRIBUTING.md'
        }
        
        doc_status = {}
        
        for doc_name, doc_path in doc_files.items():
            full_path = self.project_root / doc_path
            
            if full_path.exists():
                try:
                    content = full_path.read_text()
                    word_count = len(content.split())
                    has_substantial_content = word_count > 500
                    has_code_examples = '```' in content
                    
                    doc_status[doc_name] = {
                        'exists': True,
                        'word_count': word_count,
                        'substantial': has_substantial_content,
                        'has_examples': has_code_examples,
                        'complete': has_substantial_content
                    }
                except Exception as e:
                    doc_status[doc_name] = {
                        'exists': True,
                        'error': str(e),
                        'complete': False
                    }
            else:
                doc_status[doc_name] = {
                    'exists': False,
                    'complete': False
                }
        
        # Check examples directory
        examples_dir = self.project_root / "examples"
        if examples_dir.exists() and examples_dir.is_dir():
            example_files = list(examples_dir.glob("*.py"))
            doc_status['examples'] = {
                'exists': True,
                'count': len(example_files),
                'complete': len(example_files) > 0
            }
        else:
            doc_status['examples'] = {
                'exists': False,
                'complete': False
            }
        
        # Calculate documentation score
        complete_docs = sum(1 for item in doc_status.values() if item.get('complete', False))
        doc_score = complete_docs / len(doc_status)
        
        result['score'] = doc_score
        result['passed'] = doc_score >= 0.8
        result['details'] = doc_status
        
        return result
    
    def _validate_system_integration(self) -> Dict[str, Any]:
        """Validate overall system integration."""
        
        result = {
            'passed': False,
            'score': 0.0,
            'details': {}
        }
        
        integration_checks = {}
        
        # Check package imports
        try:
            # Check if main package can be imported
            sys.path.insert(0, str(self.project_root / "src"))
            
            # Test basic imports
            try:
                import pde_fluid_phi
                integration_checks['main_package_import'] = True
            except ImportError as e:
                integration_checks['main_package_import'] = False
                integration_checks['main_package_error'] = str(e)
            
            # Test core component imports
            core_imports = [
                'pde_fluid_phi.operators',
                'pde_fluid_phi.models',
                'pde_fluid_phi.training',
                'pde_fluid_phi.utils'
            ]
            
            successful_imports = 0
            for import_path in core_imports:
                try:
                    __import__(import_path)
                    successful_imports += 1
                    integration_checks[f'{import_path}_import'] = True
                except ImportError as e:
                    integration_checks[f'{import_path}_import'] = False
                    integration_checks[f'{import_path}_error'] = str(e)
            
            integration_checks['core_imports_ratio'] = successful_imports / len(core_imports)
            
        except Exception as e:
            integration_checks['import_test_error'] = str(e)
            integration_checks['core_imports_ratio'] = 0.0
        
        # Check file structure consistency
        structure_consistency = self._check_structure_consistency()
        integration_checks['structure_consistency'] = structure_consistency
        
        # Calculate integration score
        import_score = integration_checks.get('core_imports_ratio', 0.0)
        structure_score = 1.0 if structure_consistency else 0.5
        
        integration_score = (import_score + structure_score) / 2
        
        result['score'] = integration_score
        result['passed'] = integration_score >= 0.7
        result['details'] = integration_checks
        
        return result
    
    def _check_structure_consistency(self) -> bool:
        """Check if project structure is internally consistent."""
        
        try:
            # Check that __init__.py files exist where expected
            src_dir = self.project_root / "src" / "pde_fluid_phi"
            
            if not src_dir.exists():
                return False
            
            # Check main package __init__.py
            if not (src_dir / "__init__.py").exists():
                return False
            
            # Check subpackage __init__.py files
            subpackages = ['operators', 'models', 'training', 'utils']
            
            for subpackage in subpackages:
                subpackage_dir = src_dir / subpackage
                if subpackage_dir.exists() and subpackage_dir.is_dir():
                    if not (subpackage_dir / "__init__.py").exists():
                        return False
            
            return True
            
        except Exception:
            return False
    
    def _compute_overall_score(self):
        """Compute overall system validation score."""
        
        if not self.validation_results:
            self.overall_score = 0.0
            return
        
        # Weight different validation components
        component_weights = {
            'project_structure': 0.15,
            'core_innovations': 0.25,
            'advanced_models': 0.20,
            'optimization_systems': 0.10,
            'quality_assurance': 0.10,
            'deployment_readiness': 0.10,
            'documentation_completeness': 0.05,
            'system_integration': 0.05
        }
        
        weighted_score = 0.0
        total_weight = 0.0
        
        for component_name, component_result in self.validation_results.items():
            weight = component_weights.get(component_name, 0.05)
            score = component_result.get('score', 0.0)
            
            weighted_score += score * weight
            total_weight += weight
        
        self.overall_score = weighted_score / max(total_weight, 1.0)
    
    def _generate_final_report(self, total_time: float) -> Dict[str, Any]:
        """Generate final validation report."""
        
        # Count passed/failed components
        passed_components = sum(1 for result in self.validation_results.values() if result.get('passed', False))
        total_components = len(self.validation_results)
        
        # Generate assessment
        if self.overall_score >= 0.95:
            assessment = {
                'grade': 'A+',
                'level': 'Exceptional',
                'description': 'Outstanding implementation exceeding all expectations',
                'readiness': 'Production-ready with research-grade quality'
            }
        elif self.overall_score >= 0.90:
            assessment = {
                'grade': 'A',
                'level': 'Excellent', 
                'description': 'Excellent implementation meeting all requirements',
                'readiness': 'Production-ready'
            }
        elif self.overall_score >= 0.80:
            assessment = {
                'grade': 'B+',
                'level': 'Very Good',
                'description': 'Very good implementation with minor areas for improvement',
                'readiness': 'Near production-ready'
            }
        else:
            assessment = {
                'grade': 'B',
                'level': 'Good',
                'description': 'Good foundation with improvements needed',
                'readiness': 'Requires additional development'
            }
        
        # Identify strengths and areas for improvement
        strengths = []
        improvements = []
        
        for component_name, component_result in self.validation_results.items():
            score = component_result.get('score', 0.0)
            if score >= 0.9:
                strengths.append(f"{component_name.replace('_', ' ').title()} (score: {score:.2f})")
            elif score < 0.7:
                improvements.append(f"{component_name.replace('_', ' ').title()} (score: {score:.2f})")
        
        final_report = {
            'summary': {
                'overall_score': self.overall_score,
                'components_passed': passed_components,
                'components_total': total_components,
                'pass_rate': passed_components / max(total_components, 1),
                'validation_time': total_time,
                'timestamp': datetime.now().isoformat()
            },
            'assessment': assessment,
            'component_results': self.validation_results,
            'strengths': strengths,
            'areas_for_improvement': improvements,
            'recommendations': self._generate_recommendations()
        }
        
        return final_report
    
    def _generate_recommendations(self) -> List[str]:
        """Generate recommendations based on validation results."""
        
        recommendations = []
        
        # Check for failed components and generate specific recommendations
        for component_name, component_result in self.validation_results.items():
            if not component_result.get('passed', False):
                if component_name == 'core_innovations':
                    recommendations.append("Complete implementation of core innovations (rational-fourier operators)")
                elif component_name == 'advanced_models':
                    recommendations.append("Finalize advanced model implementations (self-healing, multi-physics)")
                elif component_name == 'optimization_systems':
                    recommendations.append("Enhance optimization systems for production scalability")
                elif component_name == 'deployment_readiness':
                    recommendations.append("Complete deployment automation and orchestration")
                elif component_name == 'documentation_completeness':
                    recommendations.append("Expand documentation with more examples and tutorials")
        
        # General recommendations based on overall score
        if self.overall_score < 0.9:
            recommendations.append("Focus on improving lowest-scoring components first")
            
        if self.overall_score >= 0.8:
            recommendations.append("Consider advanced features like quantum enhancements")
            recommendations.append("Prepare for research publication and community engagement")
        
        return recommendations
    
    def _save_validation_results(self, final_report: Dict[str, Any]):
        """Save validation results to file."""
        
        try:
            # Create validation reports directory
            reports_dir = self.project_root / "validation_reports"
            reports_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Save detailed JSON report
            json_path = reports_dir / f"system_validation_{timestamp}.json"
            with open(json_path, 'w') as f:
                json.dump(final_report, f, indent=2, default=str)
            
            # Save summary text report  
            text_path = reports_dir / f"validation_summary_{timestamp}.txt"
            with open(text_path, 'w') as f:
                self._write_text_report(f, final_report)
            
            self.logger.info(f"üìÑ Validation reports saved:")
            self.logger.info(f"   Detailed: {json_path}")
            self.logger.info(f"   Summary: {text_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to save validation results: {e}")
    
    def _write_text_report(self, file, final_report: Dict[str, Any]):
        """Write human-readable validation report."""
        
        file.write("=" * 80 + "\\n")
        file.write("PDE-FLUID-Œ¶ SYSTEM VALIDATION REPORT\\n")
        file.write("=" * 80 + "\\n\\n")
        
        summary = final_report['summary']
        assessment = final_report['assessment']
        
        # Executive summary
        file.write("EXECUTIVE SUMMARY\\n")
        file.write("-" * 17 + "\\n")
        file.write(f"Overall Score: {summary['overall_score']:.3f} ({assessment['grade']})\\n")
        file.write(f"Quality Level: {assessment['level']}\\n")
        file.write(f"System Status: {assessment['readiness']}\\n")
        file.write(f"Components Passed: {summary['components_passed']}/{summary['components_total']}\\n")
        file.write(f"Validation Time: {summary['validation_time']:.1f}s\\n\\n")
        
        # Component results
        file.write("COMPONENT VALIDATION RESULTS\\n")
        file.write("-" * 28 + "\\n")
        
        for component_name, component_result in final_report['component_results'].items():
            status = "‚úÖ PASS" if component_result.get('passed', False) else "‚ùå FAIL"
            score = component_result.get('score', 0.0)
            file.write(f"{status} {component_name.replace('_', ' ').title()}: {score:.3f}\\n")
        
        file.write("\\n")
        
        # Strengths
        if final_report['strengths']:
            file.write("SYSTEM STRENGTHS\\n")
            file.write("-" * 15 + "\\n")
            for strength in final_report['strengths']:
                file.write(f"‚Ä¢ {strength}\\n")
            file.write("\\n")
        
        # Areas for improvement
        if final_report['areas_for_improvement']:
            file.write("AREAS FOR IMPROVEMENT\\n")
            file.write("-" * 20 + "\\n")
            for improvement in final_report['areas_for_improvement']:
                file.write(f"‚Ä¢ {improvement}\\n")
            file.write("\\n")
        
        # Recommendations
        if final_report['recommendations']:
            file.write("RECOMMENDATIONS\\n")
            file.write("-" * 15 + "\\n")
            for i, rec in enumerate(final_report['recommendations'], 1):
                file.write(f"{i}. {rec}\\n")
    
    def _log_validation_summary(self, final_report: Dict[str, Any]):
        """Log validation summary to console."""
        
        summary = final_report['summary']
        assessment = final_report['assessment']
        
        self.logger.info("üèÅ SYSTEM VALIDATION COMPLETE")
        self.logger.info("=" * 60)
        self.logger.info(f"üìä Overall Score: {summary['overall_score']:.3f} ({assessment['grade']})")
        self.logger.info(f"üèÜ Quality Level: {assessment['level']}")
        self.logger.info(f"‚úÖ Components Passed: {summary['components_passed']}/{summary['components_total']}")
        self.logger.info(f"‚è±Ô∏è  Validation Time: {summary['validation_time']:.1f}s")
        
        if summary['overall_score'] >= 0.9:
            self.logger.info("üéâ EXCEPTIONAL SYSTEM - Research-grade implementation!")
        elif summary['overall_score'] >= 0.8:
            self.logger.info("üëè EXCELLENT SYSTEM - Production-ready implementation!")
        elif summary['overall_score'] >= 0.7:
            self.logger.info("üëç GOOD SYSTEM - Strong foundation with improvements needed")
        else:
            self.logger.info("‚ö†Ô∏è  SYSTEM NEEDS WORK - Focus on core implementations")


def main():
    """Main validation function."""
    
    validator = SystemValidator()
    
    try:
        final_report = validator.run_comprehensive_validation()
        
        overall_score = final_report['summary']['overall_score']
        
        if overall_score >= 0.8:
            print(f"‚úÖ SYSTEM VALIDATION PASSED - Score: {overall_score:.3f}")
            return 0
        else:
            print(f"‚ùå SYSTEM VALIDATION NEEDS IMPROVEMENT - Score: {overall_score:.3f}")
            return 1
            
    except Exception as e:
        print(f"üí• SYSTEM VALIDATION ERROR: {e}")
        return 1


if __name__ == "__main__":
    exit(main())